{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ImDuS8-PEa"
   },
   "outputs": [],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Run on Colab](https://colab.research.google.com/github/profjsb/python-seminar/blob/master/DataFiles_and_Notebooks/07_machine_learning_2/03_classification_and_CNNs_with_keras.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdonR3h-PEn"
   },
   "source": [
    "# Classification with Keras\n",
    "\n",
    "*AY 250 (UC Berkeley, 2012-2022)*\n",
    "\n",
    "We've just seen regression problems with neural nets in `keras`. Let's now explore classification, on images. Let's introduce the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist#labels) dataset: 70k small (28$\\times$28) images of 10 different types of clothing.\n",
    "\n",
    "<img src=\"https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJxs6AOJ-PEp"
   },
   "source": [
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "Tensorflow has a simple method to get this data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B6atmbVWAa7"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0o566vXl-PEq",
    "outputId": "22c635a9-370f-4dea-c0f2-60b234100a38"
   },
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from IPython.external import mathjax\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "# Print keras version\n",
    "print(\"Using tensorflow version:\", tensorflow.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmNypYL0-PEr"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb-24iwm-PEr",
    "outputId": "09803a81-ec34-41e2-945e-e5a25a28c239"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "CJhfigpH-PEu",
    "outputId": "b6cab4bb-af84-4e81-f737-f5306f185e6a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind = 21\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(x_train[ind], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RybvUvM_-PEv"
   },
   "source": [
    "To learn a model to predict the class of a given image, we could treat this 28$\\times$28 image as 1d input, like a stellar spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "zq5ew8nh-PEv",
    "outputId": "5889e16e-d3d6-4638-f8fd-f6121138df96"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(x_train[ind].reshape(-1))\n",
    "plt.ylabel(\"normalized flux\")\n",
    "plt.xlabel(\"1D pixel index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wyfn1k4-PEw"
   },
   "source": [
    "But this *clearly* jumbles the inherent spatial structure and local correlations found in natural images. Using just Dense layers in a NN we'd effectively be asking the network to learn these correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkv4gLbc-PEx"
   },
   "source": [
    "## Convolutional Neural Nets (ConvNets)\n",
    "\n",
    "NNs built for images (or more generally, inputs with spatial structure).\n",
    "\n",
    "### Key Ideas: \n",
    "  - layers see only parts of each image (effectively all other weights are zero).\n",
    "  - some layers do simple operations on previous layers to reduce dimensionality (e.g., take the largest value in a a 3x3 range)\n",
    "  - \"Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\"\n",
    " \n",
    "<img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\">\n",
    "\n",
    "\"An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input - see discussion of depth columns in text below. \"\n",
    "\n",
    "cf. http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "<img src=\"data/f2.png\">\n",
    "Source: http://www.nature.com/nature/journal/v521/n7553/fig_tab/nature14539_F2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDMy90B6-PEy"
   },
   "source": [
    "### Filter banks\n",
    "\n",
    "  http://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml0bs_8U-PEy"
   },
   "source": [
    "### Pooling\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/pool.jpeg\" width=\"40%\">\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"40%\">\n",
    "Source: http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPkNj3uh-PEz",
    "outputId": "c85a57e6-8205-43c2-f050-a8d04e346024"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "nb_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WPJO8Jw-PEz",
    "outputId": "38a5eae0-f700-4687-d3ca-517f24ba4fef"
   },
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHd9NXJ9-PEz"
   },
   "source": [
    "We want our output predictions to look like a \"probability\" of belonging to one of the 10 classes. And, importantly, we'd like to make sure that the probability over all classes sums to unity. One way to do this is to scale the outputs of the last layer using a [`softmax`](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "\n",
    "$$\n",
    "{\\rm softmax}(\\vec s) = \\frac{e^{s_i}}{\\sum_i e^{s_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTPwpm9t-PE0"
   },
   "source": [
    "So if the (unnormalized) prediction from am NN for an image is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5qSvdX5-PE0",
    "outputId": "91c68de9-e86f-4fcd-9055-a4fb0f3324b6"
   },
   "outputs": [],
   "source": [
    "s = np.random.normal(size=(10,))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb9kUxsZ-PE0"
   },
   "source": [
    "Then the softmax scaling gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9uCUoUa-PE1",
    "outputId": "a4a3d60e-65c5-4943-b955-06ad153165e0"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "print(softmax(s))\n",
    "np.testing.assert_almost_equal(softmax(s).sum(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QmXQ7xERB09",
    "outputId": "f7752a49-659e-40bc-f9a2-de6710573723"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax as sp_softmax\n",
    "sp_softmax(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZIiiTry-PE1"
   },
   "source": [
    "We'll use the \"categorical cross-entropy\" loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS3Sopro-PE2"
   },
   "source": [
    "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\">\n",
    "Source: https://gombru.github.io/2018/05/23/cross_entropy_loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0P7vAxRG-PE2",
    "outputId": "d4179869-9f68-4aed-fb95-ac165630c4bb"
   },
   "outputs": [],
   "source": [
    "# perfect match ... use a small ϵ to avoid taking log(0) since lim x log x -> 0 as x->0\n",
    "print(\"loss with a perfect match:\", -(Y_train[0] @ np.log(Y_train[0] + 1e-16)))\n",
    "print(\"loss with a predicted match:\", -(Y_train[0] @ np.log(softmax(s) + 1e-16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRlyeM7J-PE3"
   },
   "source": [
    "## Building a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNDJykCC-PE3"
   },
   "outputs": [],
   "source": [
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlhWd4qY-PE4",
    "outputId": "9296a679-e4b2-4631-a65a-60a659ab220d"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq3HJ4_h-PE4"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPZmBgBq-PE4",
    "outputId": "7caf07fb-c2da-49d9-c8d5-edb220b61af1"
   },
   "outputs": [],
   "source": [
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/ay250_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(x=x_train.reshape(-1,28,28,1), \n",
    "          y=Y_train, \n",
    "          epochs=20,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test.reshape(-1,28,28,1), Y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXWdI2td-PE5"
   },
   "source": [
    "### Aside: Dropout \n",
    "\n",
    "You'll notice above that the `accuracy` is much higher than the `val_accuracy`. That is, we overfit on the training data. One way to help protect against this is to introduce `Dropout`\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*iWQzxhVlvadk6VAJjsgXgg.png\">\n",
    "\n",
    "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014\n",
    "\n",
    "```python\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1)) # 10% of dropping an output connection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeIpKHxB-PE5"
   },
   "source": [
    "### Aside: Visualization of the layers\n",
    "\n",
    "From François Chollet (“DEEP LEARNING with Python”):\n",
    "\n",
    "Intermediate activations are “useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.”\n",
    "\n",
    "“The representations learned by convnets are highly amenable to visualization, in large part because they’re representations of visual concepts. Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.”\n",
    "\n",
    "Following from https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gw7CyEC-PE5"
   },
   "outputs": [],
   "source": [
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    keep = True\n",
    "    for t in [\"dropout\", \"normalization\", \"flatten\", \"dense\", \"activation\"]:\n",
    "        if layer.name.find(t) != -1:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name) \n",
    "        \n",
    "activation_model = tensorflow.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(x_test[0].reshape(1,28,28,1)) # Returns a list of five Numpy arrays: one array per layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uc7NSYwx-PE5",
    "outputId": "3842ae2e-1044-4b64-92b3-6ad9b11f7e70"
   },
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SRNKdFCt-PE6",
    "outputId": "0af30076-7d87-4b26-b3b8-d9c999990d34"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "images_per_row = 15\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1]   # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcFKFine-PE6"
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Another way to avoid overfitting, aside from `Dropout`, is to increase the number of exmaples used to to train the model.  Data augmentation is a generic term for methods used to expand the effect training set size by generating more data from the original training set. In images, this is pretty natural: scale changes, rotations, flips, etc. should still give us the same label. This method has the benefit of usually increasing test-time accuracy.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*C8hNiOqur4OJyEZmC7OnzQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyrkit-P-PE7"
   },
   "source": [
    "In keras we can use the `ImageDataGenerator`. In Pytorch see https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYPThwyj-PE7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# creating the data aumentation genreators for both the training images and the training label masks\n",
    "data_gen_args = dict(featurewise_center=False,\n",
    "                     featurewise_std_normalization=False,\n",
    "                     rotation_range=15.,\n",
    "                     rescale=1.0,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode = 'nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "\n",
    "image_generator = image_datagen.flow(\n",
    "    np.expand_dims(x_train, axis=3),  Y_train,\n",
    "    shuffle = True,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06gVir6C-PE7"
   },
   "outputs": [],
   "source": [
    "ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tX5CXcNK-PE7",
    "outputId": "80c2e465-4187-46ed-f9bf-85a9083ce56f"
   },
   "outputs": [],
   "source": [
    "type(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFiaNldK-PFD"
   },
   "outputs": [],
   "source": [
    "rez = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-AqMdIC-PFE",
    "outputId": "e3700653-f05d-47e5-a408-3e2808af6224"
   },
   "outputs": [],
   "source": [
    "rez[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "3BORiPwb-PFF",
    "outputId": "f6c80f65-1966-4a58-fcbc-c48e662cf2d7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(rez[0][4][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bgxRsJ1-PFG"
   },
   "outputs": [],
   "source": [
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "#tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvLXAHXG-PFG",
    "outputId": "7d5f762b-0eed-4bb4-a348-5cd14acf2473"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/ay250_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(image_generator,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test.reshape(-1,28,28,1), Y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "ZkSgsL7j-PFH",
    "outputId": "80797089-6b95-4434-f899-3518b4dec731"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "latest_log_file = !ls -t1 nn_results/training* | head -1\n",
    "latest_model_file = !ls -t1 nn_results/ay250_*.h5 | head -1\n",
    "\n",
    "hist_df = pd.read_csv(latest_log_file[0])\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2Nezvpn-PFI"
   },
   "outputs": [],
   "source": [
    "# reload the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = load_model(latest_model_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7xZi8Gb-PFJ",
    "outputId": "bd2762d4-bfde-49ff-a0e4-07634ce04c05"
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(x_test.reshape(-1, 28, 28, 1))\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDV1fUv5Tlvp",
    "outputId": "78900eaa-f4a8-422d-c70a-7f4bb4f2e5e6"
   },
   "outputs": [],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXcXzprJT_Y1",
    "outputId": "7c99e9a5-6ffa-4a14-d922-e99be4b1da1b"
   },
   "outputs": [],
   "source": [
    "ind = 1010\n",
    "\n",
    "print(pred_y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMfBliArUTg3",
    "outputId": "59eb78e4-20f6-4435-e865-432ecf535aee"
   },
   "outputs": [],
   "source": [
    "pred_y[ind].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuO0oM3AUXnR",
    "outputId": "d5ab21f1-f2c8-4fc9-da78-4693e826d642"
   },
   "outputs": [],
   "source": [
    "np.argmax(pred_y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "A4LKeEARUeOd",
    "outputId": "684db839-cd7b-4ce2-fb39-201f8aaec97f"
   },
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "_ = plt.imshow(x_test[ind], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jnVnjA6U1e-",
    "outputId": "c6cf636d-f7eb-4a13-fef9-ad98dfbf3882"
   },
   "outputs": [],
   "source": [
    "print(class_names[np.argmax(pred_y[ind])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBJPKYc-Zfcy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RDMy90B6-PEy",
    "IXWdI2td-PE5"
   ],
   "name": "02_CNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
